[net]

M=1280
height=1
filters=1280

# Transformer Block 1

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280

# Transformer Block 2

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280


# Transformer Block 3

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280


# Transformer Block 4

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280


# Transformer Block 5

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280

# Transformer Block 6

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280

# Transformer Block 7

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280


# Transformer Block 8

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280


# Transformer Block 9

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280


# Transformer Block 10

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280

# Transformer Block 11

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280

# Transformer Block 12

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280


# Transformer Block 13

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280


# Transformer Block 14

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280


# Transformer Block 15

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280

# Transformer Block 16

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280

# Transformer Block 17

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280


# Transformer Block 18

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280


# Transformer Block 19

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280


# Transformer Block 20

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280

# Transformer Block 21

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280

# Transformer Block 22

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280


# Transformer Block 23

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280


# Transformer Block 24

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280


# Transformer Block 20

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280

# Transformer Block 26

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280

# Transformer Block 27

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280


# Transformer Block 28

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280


# Transformer Block 29

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280


# Transformer Block 30

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280

# Transformer Block 31

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280

# Transformer Block 32

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280


# Transformer Block 33

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280


# Transformer Block 34

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280


# Transformer Block 35

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280

# Transformer Block 36

[layernorm]
M=1280

# Key
[matmul]
M=1280
n_head=20
n_embd=1280

# Query
[matmul]
parent=-2
M=1280
n_head=20
n_embd=1280

# Value
[matmul]
parent=-3
M=1280
n_head=20
n_embd=1280

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=20
n_embd=1280
masked=1

[attention_v]
from=-2
M=1280
n_head=20
n_embd=1280

[matmul]
M=1280

[shortcut]
from=-8
M=1280

[layernorm]
M=1280

[matmul]
M=5120
activation=gelu_acc

[matmul]
M=1280

[shortcut]
from=-4
M=1280
