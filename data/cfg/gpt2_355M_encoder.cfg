[net]

M=1024
height=1
filters=1024

# Transformer Block 1

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024

# Transformer Block 2

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024


# Transformer Block 3

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024


# Transformer Block 4

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024


# Transformer Block 5

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024

# Transformer Block 6

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024

# Transformer Block 7

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024


# Transformer Block 8

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024


# Transformer Block 9

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024


# Transformer Block 10

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024

# Transformer Block 11

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024

# Transformer Block 12

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024


# Transformer Block 13

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024


# Transformer Block 14

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024


# Transformer Block 15

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024

# Transformer Block 16

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024

# Transformer Block 17

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024


# Transformer Block 18

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024


# Transformer Block 19

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024


# Transformer Block 20

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024

# Transformer Block 21

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024

# Transformer Block 22

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024


# Transformer Block 23

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024


# Transformer Block 24

[layernorm]
M=1024

# Key
[matmul]
M=1024
n_head=16
n_embd=1024

# Query
[matmul]
parent=-2
M=1024
n_head=16
n_embd=1024

# Value
[matmul]
parent=-3
M=1024
n_head=16
n_embd=1024

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=16
n_embd=1024
masked=1

[attention_v]
from=-2
M=1024
n_head=16
n_embd=1024

[matmul]
M=1024

[shortcut]
from=-8
M=1024

[layernorm]
M=1024

[matmul]
M=4096
activation=gelu_acc

[matmul]
M=1024

[shortcut]
from=-4
M=1024
