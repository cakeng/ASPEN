[net]

M=768
height=1
filters=768

# Transformer Block 1

[layernorm]
M=768

# Key
[matmul]
M=768
n_head=12
n_embd=768

# Query
[matmul]
parent=-2
M=768
n_head=12
n_embd=768

# Value
[matmul]
parent=-3
M=768
n_head=12
n_embd=768

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=12
n_embd=768
masked=1

[attention_v]
from=-2
M=768
n_head=12
n_embd=768

[matmul]
M=768

[shortcut]
from=-8
M=768

[layernorm]
M=768

[matmul]
M=3072
activation=gelu_acc

[matmul]
M=768

[shortcut]
from=-4
M=768

# Transformer Block 2

[layernorm]
M=768

# Key
[matmul]
M=768
n_head=12
n_embd=768

# Query
[matmul]
parent=-2
M=768
n_head=12
n_embd=768

# Value
[matmul]
parent=-3
M=768
n_head=12
n_embd=768

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=12
n_embd=768
masked=1

[attention_v]
from=-2
M=768
n_head=12
n_embd=768

[matmul]
M=768

[shortcut]
from=-8
M=768

[layernorm]
M=768

[matmul]
M=3072
activation=gelu_acc

[matmul]
M=768

[shortcut]
from=-4
M=768


# Transformer Block 3

[layernorm]
M=768

# Key
[matmul]
M=768
n_head=12
n_embd=768

# Query
[matmul]
parent=-2
M=768
n_head=12
n_embd=768

# Value
[matmul]
parent=-3
M=768
n_head=12
n_embd=768

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=12
n_embd=768
masked=1

[attention_v]
from=-2
M=768
n_head=12
n_embd=768

[matmul]
M=768

[shortcut]
from=-8
M=768

[layernorm]
M=768

[matmul]
M=3072
activation=gelu_acc

[matmul]
M=768

[shortcut]
from=-4
M=768


# Transformer Block 4

[layernorm]
M=768

# Key
[matmul]
M=768
n_head=12
n_embd=768

# Query
[matmul]
parent=-2
M=768
n_head=12
n_embd=768

# Value
[matmul]
parent=-3
M=768
n_head=12
n_embd=768

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=12
n_embd=768
masked=1

[attention_v]
from=-2
M=768
n_head=12
n_embd=768

[matmul]
M=768

[shortcut]
from=-8
M=768

[layernorm]
M=768

[matmul]
M=3072
activation=gelu_acc

[matmul]
M=768

[shortcut]
from=-4
M=768


# Transformer Block 5

[layernorm]
M=768

# Key
[matmul]
M=768
n_head=12
n_embd=768

# Query
[matmul]
parent=-2
M=768
n_head=12
n_embd=768

# Value
[matmul]
parent=-3
M=768
n_head=12
n_embd=768

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=12
n_embd=768
masked=1

[attention_v]
from=-2
M=768
n_head=12
n_embd=768

[matmul]
M=768

[shortcut]
from=-8
M=768

[layernorm]
M=768

[matmul]
M=3072
activation=gelu_acc

[matmul]
M=768

[shortcut]
from=-4
M=768

# Transformer Block 6

[layernorm]
M=768

# Key
[matmul]
M=768
n_head=12
n_embd=768

# Query
[matmul]
parent=-2
M=768
n_head=12
n_embd=768

# Value
[matmul]
parent=-3
M=768
n_head=12
n_embd=768

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=12
n_embd=768
masked=1

[attention_v]
from=-2
M=768
n_head=12
n_embd=768

[matmul]
M=768

[shortcut]
from=-8
M=768

[layernorm]
M=768

[matmul]
M=3072
activation=gelu_acc

[matmul]
M=768

[shortcut]
from=-4
M=768

# Transformer Block 7

[layernorm]
M=768

# Key
[matmul]
M=768
n_head=12
n_embd=768

# Query
[matmul]
parent=-2
M=768
n_head=12
n_embd=768

# Value
[matmul]
parent=-3
M=768
n_head=12
n_embd=768

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=12
n_embd=768
masked=1

[attention_v]
from=-2
M=768
n_head=12
n_embd=768

[matmul]
M=768

[shortcut]
from=-8
M=768

[layernorm]
M=768

[matmul]
M=3072
activation=gelu_acc

[matmul]
M=768

[shortcut]
from=-4
M=768


# Transformer Block 8

[layernorm]
M=768

# Key
[matmul]
M=768
n_head=12
n_embd=768

# Query
[matmul]
parent=-2
M=768
n_head=12
n_embd=768

# Value
[matmul]
parent=-3
M=768
n_head=12
n_embd=768

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=12
n_embd=768
masked=1

[attention_v]
from=-2
M=768
n_head=12
n_embd=768

[matmul]
M=768

[shortcut]
from=-8
M=768

[layernorm]
M=768

[matmul]
M=3072
activation=gelu_acc

[matmul]
M=768

[shortcut]
from=-4
M=768


# Transformer Block 9

[layernorm]
M=768

# Key
[matmul]
M=768
n_head=12
n_embd=768

# Query
[matmul]
parent=-2
M=768
n_head=12
n_embd=768

# Value
[matmul]
parent=-3
M=768
n_head=12
n_embd=768

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=12
n_embd=768
masked=1

[attention_v]
from=-2
M=768
n_head=12
n_embd=768

[matmul]
M=768

[shortcut]
from=-8
M=768

[layernorm]
M=768

[matmul]
M=3072
activation=gelu_acc

[matmul]
M=768

[shortcut]
from=-4
M=768


# Transformer Block 10

[layernorm]
M=768

# Key
[matmul]
M=768
n_head=12
n_embd=768

# Query
[matmul]
parent=-2
M=768
n_head=12
n_embd=768

# Value
[matmul]
parent=-3
M=768
n_head=12
n_embd=768

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=12
n_embd=768
masked=1

[attention_v]
from=-2
M=768
n_head=12
n_embd=768

[matmul]
M=768

[shortcut]
from=-8
M=768

[layernorm]
M=768

[matmul]
M=3072
activation=gelu_acc

[matmul]
M=768

[shortcut]
from=-4
M=768

# Transformer Block 11

[layernorm]
M=768

# Key
[matmul]
M=768
n_head=12
n_embd=768

# Query
[matmul]
parent=-2
M=768
n_head=12
n_embd=768

# Value
[matmul]
parent=-3
M=768
n_head=12
n_embd=768

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=12
n_embd=768
masked=1

[attention_v]
from=-2
M=768
n_head=12
n_embd=768

[matmul]
M=768

[shortcut]
from=-8
M=768

[layernorm]
M=768

[matmul]
M=3072
activation=gelu_acc

[matmul]
M=768

[shortcut]
from=-4
M=768

# Transformer Block 12

[layernorm]
M=768

# Key
[matmul]
M=768
n_head=12
n_embd=768

# Query
[matmul]
parent=-2
M=768
n_head=12
n_embd=768

# Value
[matmul]
parent=-3
M=768
n_head=12
n_embd=768

[attention_k]
parent=-2
from=-3
M=1
K=64
n_head=12
n_embd=768
masked=1

[attention_v]
from=-2
M=768
n_head=12
n_embd=768

[matmul]
M=768

[shortcut]
from=-8
M=768

[layernorm]
M=768

[matmul]
M=3072
activation=gelu_acc

[matmul]
M=768

[shortcut]
from=-4
M=768
